{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set with 20 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREDIT TO :  Magnus Erik Hvass Pedersen for underlying code; \n",
    "some changes made for specific purposes of this presentation\n",
    "^Primary change was adjusting the source and target languages to EN-ES instead of DA-EN and performed an experiment to see how the training size of the model affected \n",
    "the translation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Keras\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language translation dataset from the European parliament\n",
    "# Note: first run \"pip install unidecode\"\n",
    "import europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for europarl.py\n",
    "language_code='es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for europarl.py load_data fn\n",
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data folder path location\n",
    "data_dir = \"data/europarl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "# Download texts from online, if they have not already been downloaded\n",
    "europarl.maybe_download_and_extract(language_code = language_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the texts for the source-language (English)\n",
    "data_src = europarl.load_data(english = True,\n",
    "                              language_code = language_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the texts for the destination-language (Spanish)\n",
    "data_dest = europarl.load_data(english = False,\n",
    "                               language_code = language_code,\n",
    "                               start = mark_start,\n",
    "                               end = mark_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why are no-smoking areas not enforced?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example source data (English)\n",
    "data_src[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss ¿Por qué no se respetan las áreas de no fumadores? eeee'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example destination data (Spanish)\n",
    "data_dest[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965734 1965734\n"
     ]
    }
   ],
   "source": [
    "# Size of datasets = 1,965,734 lines\n",
    "print(len(data_src), len(data_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks cannot work directly with text data\n",
    "--> Tokenize the data by assigning unique integer-tokens to each word\n",
    "--> Convert the integers to floating-point number arrays- the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add functions to Keras' Tokenizer-class by wrapping it...\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words = None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "\n",
    "        Tokenizer.__init__(self, num_words = num_words)\n",
    "\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "        # Convert all texts to lists of integer-tokens.\n",
    "        # Note that the sequences may have different lengths.\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "        \n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Max number of tokens to use in all sequences.\n",
    "        # We will pad / truncate all sequences to this length.\n",
    "        # This is a compromise so we save a lot of memory and\n",
    "        # only have to truncate maybe 5% of all the sequences.\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        # Pad / truncate all token-sequences to the given length.\n",
    "        # This creates a 2-dim numpy matrix that is easier to use.\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen = self.max_tokens,\n",
    "                                           padding = padding,\n",
    "                                           truncating = truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "        \n",
    "    def text_to_tokens(self, text, reverse = False, padding = False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to tokens. Note that we assume there is only\n",
    "        # a single text-string so we wrap it in a list.\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis = 1)\n",
    "\n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen = self.max_tokens,\n",
    "                                   padding = 'pre',\n",
    "                                   truncating = truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a tokenizer for the source-language. Note that we pad zeros at the beginning \n",
    "('pre') of the sequences. We also reverse the sequences of tokens because the research \n",
    "literature suggests that this might improve performance, because the last words seen \n",
    "by the encoder match the first words produced by the decoder, so short-term dependencies \n",
    "are supposedly modelled more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 1.43 s, total: 1min 55s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_src = TokenizerWrap(texts = data_src,\n",
    "                              padding = 'pre',\n",
    "                              reverse = True,\n",
    "                              num_words = num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the tokenizer for the destination language. We need a tokenizer \n",
    "for both the source- and destination-languages because their vocabularies are \n",
    "different. Note that this tokenizer does not reverse the sequences and it pads \n",
    "zeros at the end ('post') of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 26s, sys: 987 ms, total: 2min 27s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_dest = TokenizerWrap(texts = data_dest,\n",
    "                               padding = 'post',\n",
    "                               reverse = False,\n",
    "                               num_words = num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define convenience variables for the padded token sequences. These are just 2-dimensional numpy arrays of integer-tokens. \n",
    "\n",
    "Note that the sequence-lengths are different for the source and destination languages. This is because texts with the same meaning may have different numbers of words in the two languages. \n",
    "\n",
    "Furthermore, we have made a compromise when tokenizing the original texts in order to save a lot of memory. This means we only truncate about 5% of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1965734, 54)\n",
      "(1965734, 58)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the integer-token used to mark the beginning of a text in the destination-language.\n",
    "#(Reverse of ssss)\n",
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the integer-token used to mark the end of a text in the destination-language.\n",
    "#(Reverse of eeee)\n",
    "token_end  = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0, 3934,   19,  245, 3581,   68,   16,  175],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Token version of the earlier line that I looked at:\n",
    "#Remember- I input the source language texts in reverse order (hence the padding in front)\n",
    "tokens_src[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enforced not areas smoking no are why'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplified by reverse-engineering the words from the stored tokens:\n",
    "tokenizer_src.tokens_to_string(tokens_src[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  989,  186,   15,   13, 3902,   11, 1528,    1,   15, 8655,\n",
       "          4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token version of the earlier line that I looked at:\n",
    "tokens_dest[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss ¿por qué no se respetan las áreas de no fumadores eeee'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And again, reverse-engineer the stored tokens in the destination language (Spanish)\n",
    "tokenizer_dest.tokens_to_string(tokens_dest[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data - this is where the experiment starts! We can train with 10,000 lines, \n",
    "then 100,000, and then 1,000,000 lines and see how well the model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I store the size of the model and validation set\n",
    "model_size = 10000\n",
    "validation_set = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using the ENTIRE DATASET (n = 100,000)\n",
    "encoder_input_data = tokens_src[0:model_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output data for the decoder is identical, except shifted one time-step. We can use the same numpy array to save memory by slicing it, which merely creates different 'views' of the same data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 57)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = tokens_dest[0:model_size, :-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 57)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data = tokens_dest[0:model_size, 1:]\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, these token-sequences are identical except they are shifted one time-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  989,  186,   15,   13, 3902,   11, 1528,    1,   15, 8655,\n",
       "          4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 989,  186,   15,   13, 3902,   11, 1528,    1,   15, 8655,    4,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the tokenizer to convert these sequences back into text, we see that they are identical except for the first word which is 'ssss' that marks the beginning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss ¿por qué no se respetan las áreas de no fumadores eeee'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿por qué no se respetan las áreas de no fumadores eeee'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Neural Network\n",
    "\n",
    "Create the Encoder\n",
    "First we create the encoder-part of the neural network which maps a sequence of integer-tokens to a \"thought vector\". \n",
    "\n",
    "This is the input for the encoder which takes batches of integer-token sequences. The None indicates that the sequences can have arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape = (None, ), name ='encoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the length of the vectors output by the embedding-layer, which maps integer-tokens to vectors of values roughly between -1 and 1, so that words that have similar semantic meanings are mapped to vectors that are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the embedding-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = Embedding(input_dim = num_words,\n",
    "                              output_dim = embedding_size,\n",
    "                              name = 'encoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the size of the internal states of the Gated Recurrent Units (GRU). The same size is used in both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the 3 GRU layers that will map from a sequence of embedding-vectors to a single \"thought vector\" which summarizes the contents of the input-text. Note that the last GRU-layer does not return a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru1 = GRU(state_size, name = 'encoder_gru1',\n",
    "                   return_sequences = True)\n",
    "encoder_gru2 = GRU(state_size, name = 'encoder_gru2',\n",
    "                   return_sequences = True)\n",
    "encoder_gru3 = GRU(state_size, name = 'encoder_gru3',\n",
    "                   return_sequences = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper-function connects all the layers of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "    \n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the encoder uses the normal output from its last GRU-layer as the \"thought vector\". Research papers often use the internal state of the encoder's last recurrent layer as the \"thought vector\". But this makes the implementation more complicated and is not necessary when using the GRU.\n",
    "\n",
    "We can now use this function to connect all the layers in the encoder so it can be connected to the decoder further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Decoder\n",
    "Create the decoder-part which maps the \"thought vector\" to a sequence of integer-tokens.\n",
    "\n",
    "The decoder takes two inputs. First it needs the \"thought vector\" produced by the encoder which summarizes the contents of the input-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = Input(shape = (state_size,),\n",
    "                              name  = 'decoder_initial_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder also needs a sequence of integer-tokens as inputs...\n",
    "\n",
    "...CP: Return here.\n",
    "\n",
    "...During training we will supply this with a full sequence of integer-tokens e.g. corresponding to the text \"ssss once upon a time eeee\".\n",
    "\n",
    "During inference when we are translating new input-texts, we will start by feeding a sequence with just one integer-token for \"ssss\" which marks the beginning of a text, and combined with the \"thought vector\" from the encoder, the decoder will hopefully be able to produce the correct next word e.g. \"once\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, ), name = 'decoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the embedding-layer which converts integer-tokens to vectors of real-valued numbers roughly between -1 and 1. Note that we have different embedding-layers for the encoder and decoder because we have two different vocabularies and two different tokenizers for the source and destination languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim = num_words,\n",
    "                              output_dim = embedding_size,\n",
    "                              name = 'decoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the 3 GRU layers of the decoder. Note that they all return sequences because we ultimately want to output a sequence of integer-tokens that can be converted into a text-sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size, name = 'decoder_gru1',\n",
    "                   return_sequences = True)\n",
    "decoder_gru2 = GRU(state_size, name = 'decoder_gru2',\n",
    "                   return_sequences = True)\n",
    "decoder_gru3 = GRU(state_size, name = 'decoder_gru3',\n",
    "                   return_sequences = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU layers output a tensor with shape [batch_size, sequence_length, state_size], where each \"word\" is encoded as a vector of length state_size. We need to convert this into sequences of integer-tokens that can be interpreted as words from our vocabulary.\n",
    "\n",
    "One way of doing this is to convert the GRU output to a one-hot encoded array. It works but it is extremely wasteful, because for a vocabulary of e.g. 10000 words we need a vector with 10000 elements, so we can select the index of the highest element to be the integer-token.\n",
    "\n",
    "Note that the activation-function is set to linear instead of softmax as we would normally use for one-hot encoded outputs, because there is apparently a bug in Keras so we need to make our own loss-function, as described in detail further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words,\n",
    "                      activation = 'linear',\n",
    "                      name = 'decoder_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is built using the functional API of Keras, which allows more flexibility in connecting the layers e.g. to route different inputs to the decoder. This is useful because we have to connect the decoder directly to the encoder, but we will also connect the decoder to another input so we can run it separately.\n",
    "\n",
    "This function connects all the layers of the decoder to some input of the initial-state values for the GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state = initial_state)\n",
    "    net = decoder_gru2(net, initial_state = initial_state)\n",
    "    net = decoder_gru3(net, initial_state = initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect and Create the Models\n",
    "We can now connect the encoder and decoder in different ways.\n",
    "\n",
    "First we connect the encoder directly to the decoder so it is one whole model that can be trained end-to-end. This means the initial-state of the decoder's GRU units are set to the output of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state = encoder_output)\n",
    "\n",
    "model_train = Model(inputs = [encoder_input, decoder_input],\n",
    "                    outputs = [decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model for just the encoder alone. This is useful for mapping a sequence of integer-tokens to a \"thought-vector\" summarizing its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs = [encoder_input],\n",
    "                      outputs = [encoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a model for just the decoder alone. This allows us to directly input the initial state for the decoder's GRU units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state = decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs = [decoder_input, decoder_initial_state],\n",
    "                      outputs = [decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that all these models use the same weights and variables of the encoder and decoder. We are merely changing how they are connected. So once the entire model has been trained, we can run the encoder and decoder models separately with the trained weights.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function\n",
    "\n",
    "The output of the decoder is a sequence of one-hot encoded arrays. In order to train the decoder we need to supply the one-hot encoded arrays that we desire to see on the decoder's output, and then use a loss-function like cross-entropy to train the decoder to produce this desired output.\n",
    "\n",
    "However, our data-set contains integer-tokens instead of one-hot encoded arrays. Each one-hot encoded array has 10000 elements so it would be extremely wasteful to convert the entire data-set to one-hot encoded arrays.\n",
    "\n",
    "A better way is to use a so-called sparse cross-entropy loss-function, which does the conversion internally from integers to one-hot encoded arrays. \n",
    "\n",
    "This is done with a sparse-cross-entropy function directly from TensorFlow.\n",
    "\n",
    "Firstly, the loss-function calculates the softmax internally to improve numerical stability - this is why we used a linear activation function in the last dense-layer of the decoder-network above.\n",
    "\n",
    "Secondly, the loss-function from TensorFlow will output a 2-rank tensor of shape [batch_size, sequence_length] given these inputs. But this must ultimately be reduced to a single scalar-value whose gradient can be derived by TensorFlow so it can be optimized using gradient descent. Keras supports some weighting of loss-values across the batch but the semantics are unclear so to be sure that we calculate the loss-function across the entire batch and across the entire sequences, we manually calculate the loss average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y_true,\n",
    "                                                          logits = y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually create a placeholder variable for the decoder's output. The shape is set to (None, None) which means the batch can have an arbitrary number of sequences, which can have an arbitrary number of integer-tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype = 'int32', shape = (None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile the model using our custom loss-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer = optimizer,\n",
    "                    loss=sparse_cross_entropy,\n",
    "                    target_tensors = [decoder_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback Functions\n",
    "\n",
    "During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n",
    "\n",
    "This is the callback for writing checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = '21_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath = path_checkpoint,\n",
    "                                      monitor = 'val_loss',\n",
    "                                      verbose = 1,\n",
    "                                      save_weights_only = True,\n",
    "                                      save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the callback for stopping the optimization when performance worsens on the validation-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                                        patience = 3, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the callback for writing the TensorBoard log during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir = './21_logs/',\n",
    "                                   histogram_freq = 0,\n",
    "                                   write_graph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Checkpoint\n",
    "\n",
    "You can reload the last saved checkpoint so you don't have to train the model every time you want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model\n",
    "\n",
    "We wrap the data in named dicts so we are sure the data is assigned correctly to the inputs and outputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = \\\n",
    "{\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = \\\n",
    "{\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a validation-set of 1000 sequences but Keras needs this number as a fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_split = validation_set / len(encoder_input_data)\n",
    "validation_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model. One epoch of training took about 1 hour on a GTX 1070 GPU. You probably need to run 10 epochs or more during training. After 10 epochs the loss was about 1.10 on the training-set and about 1.15 on the validation-set.\n",
    "\n",
    "Update: Did the same model size (10000) with 20 epochs, and took about 4 hours with a loss of 1.934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9900 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "9900/9900 [==============================] - 670s 68ms/step - loss: 2.6612 - val_loss: 2.3027\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30269, saving model to 21_checkpoint.keras\n",
      "Epoch 2/20\n",
      "9900/9900 [==============================] - 683s 69ms/step - loss: 2.5650 - val_loss: 2.2706\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.30269 to 2.27062, saving model to 21_checkpoint.keras\n",
      "Epoch 3/20\n",
      "9900/9900 [==============================] - 669s 68ms/step - loss: 2.4913 - val_loss: 2.2353\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.27062 to 2.23535, saving model to 21_checkpoint.keras\n",
      "Epoch 4/20\n",
      "9900/9900 [==============================] - 658s 66ms/step - loss: 2.4500 - val_loss: 2.1994\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.23535 to 2.19942, saving model to 21_checkpoint.keras\n",
      "Epoch 5/20\n",
      "9900/9900 [==============================] - 681s 69ms/step - loss: 2.4043 - val_loss: 2.1794\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.19942 to 2.17938, saving model to 21_checkpoint.keras\n",
      "Epoch 6/20\n",
      "9900/9900 [==============================] - 662s 67ms/step - loss: 2.3737 - val_loss: 2.1707\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.17938 to 2.17066, saving model to 21_checkpoint.keras\n",
      "Epoch 7/20\n",
      "9900/9900 [==============================] - 674s 68ms/step - loss: 2.3310 - val_loss: 2.1517\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.17066 to 2.15168, saving model to 21_checkpoint.keras\n",
      "Epoch 8/20\n",
      "9900/9900 [==============================] - 677s 68ms/step - loss: 2.3053 - val_loss: 2.1347\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.15168 to 2.13473, saving model to 21_checkpoint.keras\n",
      "Epoch 9/20\n",
      "9900/9900 [==============================] - 676s 68ms/step - loss: 2.2694 - val_loss: 2.1102\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.13473 to 2.11020, saving model to 21_checkpoint.keras\n",
      "Epoch 10/20\n",
      "9900/9900 [==============================] - 679s 69ms/step - loss: 2.2339 - val_loss: 2.0728\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.11020 to 2.07279, saving model to 21_checkpoint.keras\n",
      "Epoch 11/20\n",
      "9900/9900 [==============================] - 684s 69ms/step - loss: 2.2009 - val_loss: 2.0403\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.07279 to 2.04030, saving model to 21_checkpoint.keras\n",
      "Epoch 12/20\n",
      "9900/9900 [==============================] - 700s 71ms/step - loss: 2.1702 - val_loss: 2.0177\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.04030 to 2.01767, saving model to 21_checkpoint.keras\n",
      "Epoch 13/20\n",
      "9900/9900 [==============================] - 684s 69ms/step - loss: 2.1409 - val_loss: 2.0063\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.01767 to 2.00629, saving model to 21_checkpoint.keras\n",
      "Epoch 14/20\n",
      "9900/9900 [==============================] - 686s 69ms/step - loss: 2.1127 - val_loss: 1.9843\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.00629 to 1.98434, saving model to 21_checkpoint.keras\n",
      "Epoch 15/20\n",
      "9900/9900 [==============================] - 722s 73ms/step - loss: 2.0882 - val_loss: 1.9744\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.98434 to 1.97438, saving model to 21_checkpoint.keras\n",
      "Epoch 16/20\n",
      "9900/9900 [==============================] - 726s 73ms/step - loss: 2.0504 - val_loss: 1.9585\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.97438 to 1.95851, saving model to 21_checkpoint.keras\n",
      "Epoch 17/20\n",
      "9900/9900 [==============================] - 699s 71ms/step - loss: 2.0341 - val_loss: 1.9590\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.95851\n",
      "Epoch 18/20\n",
      "9900/9900 [==============================] - 694s 70ms/step - loss: 2.0010 - val_loss: 1.9341\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.95851 to 1.93411, saving model to 21_checkpoint.keras\n",
      "Epoch 19/20\n",
      "9900/9900 [==============================] - 755s 76ms/step - loss: 1.9832 - val_loss: 1.9436\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.93411\n",
      "Epoch 20/20\n",
      "9900/9900 [==============================] - 801s 81ms/step - loss: 1.9565 - val_loss: 1.9416\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.93411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ca6e1438>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(x = x_data,\n",
    "                y = y_data,\n",
    "                batch_size = 640,\n",
    "                epochs = 20,\n",
    "                validation_split = validation_split,\n",
    "                callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate Texts\n",
    "\n",
    "This function translates a text from the source-language to the destination-language and optionally prints a true translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text = None):\n",
    "    \"\"\"Translate a single text-string.\"\"\"\n",
    "\n",
    "    # Convert the input-text to integer-tokens.\n",
    "    # Note the sequence of tokens has to be reversed.\n",
    "    # Padding is probably not necessary.\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text = input_text,\n",
    "                                                reverse = True,\n",
    "                                                padding = True)\n",
    "    \n",
    "    # Get the output of the encoder's GRU which will be\n",
    "    # used as the initial state in the decoder's GRU.\n",
    "    # This could also have been the encoder's final state\n",
    "    # but that is really only necessary if the encoder\n",
    "    # and decoder use the LSTM instead of GRU because\n",
    "    # the LSTM has two internal states.\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens,\n",
    "    # but the decoder-model expects a batch of sequences.\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape = shape, dtype = np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for ' eeee'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "        \n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        # Note that we input the entire sequence of tokens\n",
    "        # to the decoder. This wastes a lot of computation\n",
    "        # because we are only interested in the last input\n",
    "        # and output. We could modify the code to return\n",
    "        # the GRU-states when calling predict() and then\n",
    "        # feeding these GRU-states as well the next time\n",
    "        # we call predict(), but it would make the code\n",
    "        # much more complicated.\n",
    "\n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    # Print the input-text.\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    # Print the translated output-text.\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    # Optionally print the true translated text.\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Why are no-smoking areas not enforced?\n",
      "\n",
      "Translated text:\n",
      " es una cuestión de la comisión eeee\n",
      "\n",
      "True output text:\n",
      "ssss ¿Por qué no se respetan las áreas de no fumadores? eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I return to the example from earlier to see how well it does...\n",
    "idx = 45\n",
    "translate(input_text = data_src[idx],\n",
    "          true_output_text = data_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "We therefore respect whatever Parliament may decide.\n",
      "\n",
      "Translated text:\n",
      " es una cuestión de la comisión eeee\n",
      "\n",
      "True output text:\n",
      "ssss Por lo tanto, nosotros respetamos lo que en este sentido pueda decidir el Parlamento. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 86\n",
    "translate(input_text = data_src[idx],\n",
    "          true_output_text = data_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Yes, Mrs Schroedter, I shall be pleased to look into the facts of this case when I have received your letter.\n",
      "\n",
      "Translated text:\n",
      " en este sentido no se ha hecho en el parlamento de la comisión de la comisión de la comisión de la comisión de la comisión y la comisión de la comisión de la comisión y la comisión de la comisión de la comisión y la comisión de la comisión de la comisión de la comisión y la comisión\n",
      "\n",
      "True output text:\n",
      "ssss Sí, señora Schroedter, de buena gana voy a examinar los hechos relacionados con este tema en cuanto reciba su carta. eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 32\n",
    "translate(input_text = data_src[idx],\n",
    "          true_output_text = data_dest[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
